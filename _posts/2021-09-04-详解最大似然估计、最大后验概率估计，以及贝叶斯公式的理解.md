# 详解最大似然估计、最大后验概率估计，以及贝叶斯公式的理解

我们先从概率和统计的区别讲起。

## 概率和统计的区别

概率（probability）和统计（statistic）看上去是两个相近的概念，其实研究的问题刚刚好相反。

概率研究的问题是，已知数据的模型和参数，怎么预测这个模型产生的结果，包括研究这些结果的特性(均值，方差，协方差等等)。

统计研究的问题则相反。在实际中我们观察或者测试得到一组数据，要利用这组数据去预测模型和参数。找到最好最适合这些数据的模型（高斯分布，泊松分布，指数分布，拉普拉斯分布等等）

一句话总结：**概率是已知模型和参数，推数据；统计是已知数据，推模型和参数**。

本文中的最大似然估计（MLE）和最大后验估计（MAP）都是统计领域的问题。都是用来推测参数的方法，为什么会有两种不同的方法，我们先来看贝叶斯公式。

## 贝叶斯公式

> **贝叶斯定理**（英语：Bayes' theorem）是[概率论](https://zh.wikipedia.org/wiki/概率論)中的一个[定理](https://zh.wikipedia.org/wiki/定理)，描述在已知一些条件下，某[事件](https://zh.wikipedia.org/wiki/事件_(概率论))的发生概率。比如，如果已知某人妈妈得癌症与寿命有关，使用贝叶斯定理则可以通过得知某人年龄，来更加准确地计算出他妈妈罹患癌症的概率。
>
> 通常，事件A在事件B已发生的条件下发生的概率，与事件B在事件A已发生的条件下发生的概率是不一样的。然而，这两者是有确定的关系的，贝叶斯定理就是这种关系的陈述。贝叶斯公式的一个用途，即透过已知的三个概率而推出第四个概率。贝叶斯定理跟[随机变量](https://zh.wikipedia.org/wiki/隨機變量)的[条件概率](https://zh.wikipedia.org/wiki/條件機率)以及[边际概率分布](https://zh.wikipedia.org/wiki/联合分布)有关。
>
> 作为一个普遍的原理，贝叶斯定理对于所有概率的解释是有效的。这一定理的主要应用为[贝叶斯推断](https://zh.wikipedia.org/wiki/贝叶斯推断)，是[推论统计学](https://zh.wikipedia.org/wiki/推論統計學)中的一种推断法。这一定理名称来自于[托马斯·贝叶斯](https://zh.wikipedia.org/wiki/托马斯·贝叶斯)。

### 贝叶斯公式的定义

贝叶斯定理是关于随机事件A和B的[条件概率](https://zh.wikipedia.org/wiki/条件概率)的一则定理。

![{\displaystyle P(A\mid B)={\frac {P(A)P(B\mid A)}{P(B)}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e08d4ab0386c0ebb7d87f398cd38f911440fe3da)

其中**A**以及**B**为随机事件，且[P(B)](https://wikimedia.org/api/rest_v1/media/math/render/svg/e593d180a26fd68657ea50368dbfe1a661e652aa)不为零。![{\displaystyle P(A\mid B)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8f8f30f4da85b53901e0871eb41ed8827f511bb7)是指在事件![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)发生的情况下事件![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)发生的概率。前提是两个事件发生的概率有联系。

在贝叶斯定理中，每个名词都有约定俗成的名称：

- ![{\displaystyle P(A\mid B)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/8f8f30f4da85b53901e0871eb41ed8827f511bb7)是已知![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)发生后，![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)的条件概率]。也称作![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)的[后验概率](https://zh.wikipedia.org/wiki/后验概率)。
- ![P(A)](https://wikimedia.org/api/rest_v1/media/math/render/svg/4f264d19e21604793c6dc54f8044df454db82744)是![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)的[先验概率](https://zh.wikipedia.org/wiki/先验概率)（或[边缘概率](https://zh.wikipedia.org/wiki/边缘概率)）。其不考虑任何![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)方面的因素。
- ![{\displaystyle P(B\mid A)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e2fe9ad0fdfd8920e56ca948400e111852af0665)是已知![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)发生后，![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)的条件概率。也可称为![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)的后验概率。某些文献又称其为在特定![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)时，![A](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)的[似然性](https://zh.wikipedia.org/wiki/似然函数)，因为![{\displaystyle P(B\mid A)=L(A\mid B)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/28cfc18ecc74984b90db23005914dcb649b7e30e)。
- ![P(B)](https://wikimedia.org/api/rest_v1/media/math/render/svg/e593d180a26fd68657ea50368dbfe1a661e652aa)是![B](https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a)的[先验概率](https://zh.wikipedia.org/wiki/先验概率)。

按这些术语，贝叶斯定理可表述为：

​	后验概率 = (似然性*先验概率)/标准化常量

也就是说，后验概率与先验概率和相似度的乘积成正比。

另外，比例![{\displaystyle P(B|A)/P(B)}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ac57e2dbf1c0bc467a4c8d80da217924d22bff66)也有时被称作标准似然度（standardised likelihood)，贝叶斯定理可表述为：

​	后验概率 = 标准似然度*先验概率

### 二选一的形式

贝叶斯理论通常可以再写成下面的形式：

![P(B)=P(A,B)+P(A^{C},B)=P(B|A)P(A)+P(B|A^{C})P(A^{C})](https://wikimedia.org/api/rest_v1/media/math/render/svg/4da5a989fc6ee3c6ece9478f476af4a1e98db269)

其中$A^C$是A的[补集](https://zh.wikipedia.org/wiki/補集)（即非A）。故上式亦可写成：

![{\displaystyle P(A|B)={\frac {P(B|A)\,P(A)}{P(B|A)P(A)+P(B|A^{C})P(A^{C})}}\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f52d6b5876b084d1bc6f467f56995c00a45946b5)

在更一般化的情况，假设${A_i}$是事件集合里的部分集合，对于任意的${A_i}$ ，贝叶斯理论可用下式表示：

![{\displaystyle P(A_{i}|B)={\frac {P(B|A_{i})\,P(A_{i})}{\sum _{j}P(B|A_{j})\,P(A_{j})}}\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4b66ebf00ba50f5f3a16b61f83fd2304022119d6)

**贝叶斯公式就是在描述，你有多大把握能相信一件证据？（how much you can trust the evidence）**
$$
P(B|A^{C})P(A^{C})=0
$$

## 似然函数

似然（likelihood）这个词其实和概率（probability）是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近就是了）。

对于这个函数：
$$
P(x|\theta)
$$
输入有两个：x表示某一个具体的数据；θ表示模型的参数。

1. 如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。

2. 如果x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。

比如，![[公式]](https://www.zhihu.com/equation?tex=f%28x%2Cy%29%3Dx%5Ey) , 即x的y次方。如果x是已知确定的(例如x=2)，这就是 ![[公式]](https://www.zhihu.com/equation?tex=f%28y%29%3D2%5Ey) , 这是指数函数。 如果y是已知确定的(例如y=2)，这就是 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%3Dx%5E2) ，这是二次函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。

接下来我们来看看MLE和MAP是什么：

## 最大似然估计（MLE）

假设有一个造币厂生产某种硬币，现在我们拿到了一枚这种硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正反面出现的概率（记为θ）各是多少？

这是一个统计问题，回想一下，解决统计问题需要什么？ 数据！

于是我们拿这枚硬币抛了10次，得到的数据（x0 ）是：反正正正正反正正正反。我们想求的正面概率θ是模型参数，而抛硬币模型我们可以假设是 二项分布。

那么，出现实验结果x_0（即反正正正正反正正正反）的似然函数是多少呢？

$$
f(x_0 ,\theta) = (1-\theta)\times\theta\times\theta\times\theta\times\theta\times(1-\theta)\times\theta\times\theta\times\theta\times(1-\theta) = \theta ^ 7(1 -\theta)^3
$$
注意，这是个只关于θ的函数。而最大似然估计，顾名思义，就是要最大化这个函数。我们可以画出f(θ)的图像：

![likeli](https://img-blog.csdnimg.cn/img_convert/59cce91b31a30bcbff20516cf1806215.png)

可以看出，在θ=0.7时，似然函数取得最大值。

这样，我们已经完成了对θ的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。（ummm…这非常直观合理，对吧？）

且慢，一些人可能会说，硬币一般都是均匀的啊！ 就算你做实验发现结果是“反正正正正反正正正反”，我也不信θ=0.7。

这里就包含了贝叶斯学派的思想了——**要考虑先验概率**。 为此，引入了最大后验概率估计。

### 为什么在一般定义中会取对数？

如下图所示，一般定义中，极大似然估计我们会对似然函数取对数。

![image-20210904215208431](C:\Users\14497\AppData\Roaming\Typora\typora-user-images\image-20210904215208431.png)

## 最大后验概率估计

最大后验概率估计
最大似然估计是求参数θ, 使似然函数$ P(x_0|\theta) $最大 。求得的$\theta$不单单让似然函数大，$\theta$自己出现的先验概率也得大。 （这有点像正则化里加惩罚项的思想，不过正则化里是利用加法，而MAP里是利用乘法）

MAP其实是在最大化
$$
P(\theta|x_0) = \frac{P(x_0|\theta)P(\theta)}{P(x_0)}
$$
 ，不过因为$x_0$ 是确定的（即投出的“反正正正正反正正正反”），$P ( x_0 )$是一个已知值，所以去掉了分母。（假设“投10次硬币”是一次实验，实验做了1000次，“反正正正正反正正正反”出现了n次，则$P(x_0)=n/1000$。总之，这是一个可以由数据集得到的值）。最大化$P(\theta | x_0)$的意义也很明确，$x_0$已经出现了，要求$ \theta$取什么值使$ P(\theta | x_0)$最大。顺带一提，$ P(\theta | x_0)$即后验概率，这就是“最大后验概率估计”名字的由来。

对于投硬币的例子来看，我们认为（”先验地知道“）

$\theta$取0.5的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设$P(\theta)$为均值0.5，方差0.1的高斯函数，如下图：

![ptheta](https://img-blog.csdnimg.cn/img_convert/63ac4f85c2439a7167012aeca9b20a26.png)

则$P(x_0∣\theta)P(\theta)$的函数为每个事件出现的情况（一般是$P(\theta)$或者是$1-P(\theta)$）和先验函数$P(\theta)$的乘积，图像为：

![map1](https://img-blog.csdnimg.cn/img_convert/f8d0f3edd20990f27e43367b41d0c0f9.png)

注意，此时函数取最大值时，θ 取值已向左偏移，不再是0.7。实际上，在θ=0.558时函数取得了最大值。即用最大后验概率估计，得到θ=0.558

最后，那要怎样才能说服一个贝叶斯派相信θ=0.7呢？你得多做点实验。

如果做了1000次实验，其中700次都是正面向上，这时似然函数为:

![likeli2](https://img-blog.csdnimg.cn/img_convert/628f070547dd6c702e597a75d39ad743.png)



如果仍然假设P(θ)为均值0.5，方差0.1的高斯函数，$P(x_0 ∣θ)P(θ)$的函数图像为：


![map2](https://img-blog.csdnimg.cn/img_convert/7dba20cbd0a68da44a5253d889f27969.png)

在θ=0.696处，$P(x_0∣θ)P(θ)$取得最大值。

这样，就算一个考虑了先验概率的贝叶斯派，也不得不承认得把θ估计在0.7附近了。



## 参考

[详解最大似然估计，最大后验估计，贝叶斯公式](https://blog.csdn.net/u011508640/article/details/72815981)

[一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)

[贝叶斯定理](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)

[极大似然估计的理解与应用](https://www.cnblogs.com/xing901022/p/8418894.html)